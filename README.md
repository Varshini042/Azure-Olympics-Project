# Azure Olympics Data Engineering Project

## ğŸš€ Overview
This project is a complete end-to-end Azure Data Engineering workflow using tools like Azure Data Factory, Azure DevOps, Databricks, PySpark, Delta Live Tables, and Spark Structured Streaming. It includes real-time ingestion, transformation, orchestration, CI/CD implementation, and big data analytics using a Medallion Architecture.

## ğŸ”‘ Key Features
- Real-Time ETL Pipelines â€“ Using ADF and Spark Streaming.
- CI/CD with Azure DevOps â€“ Automated deployment with Git integration.
- Delta Lake & Time Travel â€“ Efficient storage with Delta tables.
- Data Orchestration â€“ Managed via Databricks Workflows.
- Dynamic Notebooks & Utilities â€“ Implemented using Databricks Utilities.
- Structured Streaming & CDC â€“ For continuous and incremental processing.

## ğŸ› ï¸ Technologies Used
- Azure Data Factory (ADF) â€“ Real-time and batch data ingestion.
- Azure DevOps â€“ CI/CD pipelines and Git integration.
- Azure Data Lake Storage Gen2 â€“ Scalable storage for structured/unstructured data.
- Azure Databricks with Unity Catalog â€“ Secure and scalable Spark-based processing.
- PySpark â€“ For large-scale transformations and analytics.
- Delta Live Tables â€“ For simplified and reliable pipelines.
- Spark Structured Streaming â€“ For real-time data ingestion.

## ğŸ—ï¸ Data Architecture
The architecture follows a modern Medallion Architecture:
1. Bronze Layer â€“ Raw data ingested from GitHub using Azure Data Factory.
2. Silver Layer â€“ Cleaned and transformed using PySpark in Databricks.
3. Gold Layer â€“ Aggregated and analytics-ready data with Delta Live Tables.
4. Streaming Layer â€“ Real-time CDC and streaming data handled by Spark Structured Streaming.
5. Orchestration â€“ Using Databricks Workflows and integrated with Azure DevOps for CI/CD.
![image](https://github.com/user-attachments/assets/10cfc478-3342-4e4c-a7f2-6e954e992ce1)


## ğŸ“ Data Sources
- GitHub Repositories â€“ Public datasets ingested via ADF.
- Olympic Dataset â€“ Sample data processed through medallion layers.

## ğŸ”„ Key Components in Action
- ğŸ§ª Data Ingestion â€“ Azure Data Factory pulls datasets dynamically from GitHub.
- ğŸ” Transformation â€“ PySpark handles complex logic for cleaning and enrichment.
- ğŸ’¾ Storage â€“ Delta Lake stores data efficiently across bronze, silver, and gold layers.
- ğŸš€ Automation â€“ DevOps pipelines enable CI/CD across environments.
- âš™ï¸ Orchestration â€“ Databricks Workflows streamline notebook scheduling and execution.
- ğŸ“Š Streaming & CDC â€“ Spark Structured Streaming enables real-time updates and Change Data Capture.

---

Happy Learning! ğŸ…

